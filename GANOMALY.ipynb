{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GANOMALY.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLMWHvyxIM8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# In[5]:\n",
        "\n",
        "\n",
        "#get_ipython().system('pip install tensorflow==1.14.0')\n",
        "\n",
        "\n",
        "# In[3]:\n",
        "\n",
        "\n",
        "from keras import layers \n",
        "import keras \n",
        "import keras.backend as K\n",
        "\n",
        "width = 32 \n",
        "height = 32 \n",
        "channels = 1\n",
        "\n",
        "input_layer = layers.Input(name='input', shape=(height, width, channels))\n",
        "#1906.11632_gan_anomaly_detection.pdf\n",
        "#Encoder\n",
        "x = layers.Conv2D(32, (5,5), strides=(1,1), padding='same', name='conv_1', kernel_regularizer = 'l2')(input_layer)\n",
        "x = layers.LeakyReLU(name='leaky_1')(x)\n",
        "\n",
        "x = layers.Conv2D(64, (3,3), strides=(2,2), padding='same', name='conv_2', kernel_regularizer = 'l2')(x)\n",
        "x = layers.BatchNormalization(name='norm_1')(x)\n",
        "x = layers.LeakyReLU(name='leaky_2')(x)\n",
        "\n",
        "x = layers.Conv2D(128, (3,3), strides=(2,2), padding='same', name='conv_3', kernel_regularizer = 'l2')(x)\n",
        "x = layers.BatchNormalization(name='norm_2')(x)\n",
        "x = layers.LeakyReLU(name='leaky_3')(x)\n",
        "\n",
        "x = layers.Conv2D(256, (3,3), strides=(2,2), padding='same', name='conv_4', kernel_regularizer = 'l2')(x)\n",
        "x = layers.BatchNormalization(name='norm_3')(x)\n",
        "x = layers.LeakyReLU(name='leaky_4')(x)\n",
        "\n",
        "x = layers.GlobalAveragePooling2D(name='g_encoder_output')(x)\n",
        "\n",
        "g_e = keras.models.Model(inputs=input_layer, outputs=x)\n",
        "#g_e.summary()\n",
        "\n",
        "input_layer = layers.Input(name='input', shape=(height, width, channels))\n",
        "#1906.11632_gan_anomaly_detection.pdf\n",
        "#encoding + decoding\n",
        "x = g_e(input_layer)\n",
        "\n",
        "y = layers.Dense(width * width * 2, name='dense')(x) # 2 = 128 / 8 / 8\n",
        "y = layers.Reshape((width//8, width//8, 128), name='de_reshape')(y)\n",
        "\n",
        "y = layers.Conv2DTranspose(128, (3,3), strides=(2,2), padding='same', name='deconv_1', kernel_regularizer = 'l2')(y)\n",
        "y = layers.LeakyReLU(name='de_leaky_1')(y)\n",
        "\n",
        "y = layers.Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', name='deconv_2', kernel_regularizer = 'l2')(y)\n",
        "y = layers.LeakyReLU(name='de_leaky_2')(y)\n",
        "\n",
        "y = layers.Conv2DTranspose(32, (3,3), strides=(2,2), padding='same', name='deconv_3', kernel_regularizer = 'l2')(y)\n",
        "y = layers.LeakyReLU(name='de_leaky_3')(y)\n",
        "\n",
        "y = layers.Conv2DTranspose(channels, (1, 1), strides=(1,1), padding='same', name='decoder_deconv_output', kernel_regularizer = 'l2', activation='tanh')(y)\n",
        "\n",
        "g = keras.models.Model(inputs=input_layer, outputs=y)\n",
        "#g.summary()\n",
        "g.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "input_layer = layers.Input(name='input', shape=(height, width, channels))\n",
        "#1906.11632_gan_anomaly_detection.pdf\n",
        "#second encoder\n",
        "z = layers.Conv2D(32, (5,5), strides=(1,1), padding='same', name='encoder_conv_1', kernel_regularizer = 'l2')(input_layer)\n",
        "z = layers.LeakyReLU()(z)\n",
        "\n",
        "z = layers.Conv2D(64, (3,3), strides=(2,2), padding='same', name='encoder_conv_2', kernel_regularizer = 'l2')(z)\n",
        "z = layers.BatchNormalization(name='encoder_norm_1')(z)\n",
        "z = layers.LeakyReLU()(z)\n",
        "\n",
        "\n",
        "z = layers.Conv2D(128, (3,3), strides=(2,2), padding='same', name='encoder_conv_3', kernel_regularizer = 'l2')(z)\n",
        "z = layers.BatchNormalization(name='encoder_norm_2')(z)\n",
        "z = layers.LeakyReLU()(z)\n",
        "\n",
        "z = layers.Conv2D(256, (3,3), strides=(2,2), padding='same', name='conv_41', kernel_regularizer = 'l2')(z)\n",
        "z = layers.BatchNormalization(name='encoder_norm_3')(z)\n",
        "z = layers.LeakyReLU()(z)\n",
        "\n",
        "z = layers.GlobalAveragePooling2D(name='encoder_output')(z)\n",
        "\n",
        "encoder = keras.models.Model(input_layer, z)\n",
        "#encoder.summary()\n",
        "\n",
        "input_layer = layers.Input(name='input', shape=(height, width, channels))\n",
        "#feature extraction layer\n",
        "f = layers.Conv2D(32, (5,5), strides=(1,1), padding='same', name='f_conv_1', kernel_regularizer = 'l2')(input_layer)\n",
        "f = layers.LeakyReLU(name='f_leaky_1')(f)\n",
        "\n",
        "f = layers.Conv2D(64, (3,3), strides=(2,2), padding='same', name='f_conv_2', kernel_regularizer = 'l2')(f)\n",
        "f = layers.BatchNormalization(name='f_norm_1')(f)\n",
        "f = layers.LeakyReLU(name='f_leaky_2')(f)\n",
        "\n",
        "\n",
        "f = layers.Conv2D(128, (3,3), strides=(2,2), padding='same', name='f_conv_3', kernel_regularizer = 'l2')(f)\n",
        "f = layers.BatchNormalization(name='f_norm_2')(f)\n",
        "f = layers.LeakyReLU(name='f_leaky_3')(f)\n",
        "\n",
        "\n",
        "f = layers.Conv2D(128, (3,3), strides=(2,2), padding='same', name='f_conv_4', kernel_regularizer = 'l2')(f)\n",
        "f = layers.BatchNormalization(name='f_norm_3')(f)\n",
        "f = layers.LeakyReLU(name='feature_output')(f)\n",
        "\n",
        "feature_extractor = keras.models.Model(input_layer, f)\n",
        "#feature_extractor.summary()\n",
        "\n",
        "class AdvLoss(keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AdvLoss, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        ori_feature = feature_extractor(x[0])\n",
        "        gan_feature = feature_extractor(x[1])\n",
        "        return K.mean(K.square(ori_feature - K.mean(gan_feature, axis=0)))\n",
        "\n",
        "    def get_output_shape_for(self, input_shape):\n",
        "        return (input_shape[0][0], 1)\n",
        "    \n",
        "class CntLoss(keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(CntLoss, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        ori = x[0]\n",
        "        gan = x[1]\n",
        "        return K.mean(K.abs(ori - gan))\n",
        "\n",
        "    def get_output_shape_for(self, input_shape):\n",
        "        return (input_shape[0][0], 1)\n",
        "    \n",
        "class EncLoss(keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(EncLoss, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        ori = x[0]\n",
        "        gan = x[1]\n",
        "        return K.mean(K.square(g_e(ori) - encoder(gan)))\n",
        "\n",
        "    def get_output_shape_for(self, input_shape):\n",
        "        return (input_shape[0][0], 1)\n",
        "\n",
        "# model for training\n",
        "input_layer = layers.Input(name='input', shape=(height, width, channels))\n",
        "gan = g(input_layer) # g(x)\n",
        "\n",
        "adv_loss = AdvLoss(name='adv_loss')([input_layer, gan])\n",
        "cnt_loss = CntLoss(name='cnt_loss')([input_layer, gan])\n",
        "enc_loss = EncLoss(name='enc_loss')([input_layer, gan])\n",
        "\n",
        "gan_trainer = keras.models.Model(input_layer, [adv_loss, cnt_loss, enc_loss])\n",
        "\n",
        "# loss function\n",
        "def loss(yt, yp):\n",
        "    return yp\n",
        "\n",
        "losses = {\n",
        "    'adv_loss': loss,\n",
        "    'cnt_loss': loss,\n",
        "    'enc_loss': loss,\n",
        "}\n",
        "\n",
        "lossWeights = {'cnt_loss': 20.0, 'adv_loss': 1.0, 'enc_loss': 1.0}\n",
        "\n",
        "# compile\n",
        "gan_trainer.compile(optimizer = 'adam', loss=losses, loss_weights=lossWeights)\n",
        "#gan_trainer.summary()\n",
        "\n",
        "input_layer = layers.Input(name='input', shape=(height, width, channels))\n",
        "\n",
        "f = feature_extractor(input_layer)\n",
        "\n",
        "d = layers.GlobalAveragePooling2D(name='glb_avg')(f)\n",
        "d = layers.Dense(1, activation='sigmoid', name='d_out')(d)\n",
        "\n",
        "d = keras.models.Model(input_layer, d)\n",
        "#d.summary()\n",
        "\n",
        "d.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from sklearn import preprocessing\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "#(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "#x_ok = x_train[y_train == 1] # 6742 筆\n",
        "#x_test = x_test[(y_test == 7) | (y_test == 1)] # 1135 筆 \"1\", 1028 筆 \"7\"\n",
        "#y_test = y_test[(y_test == 7) | (y_test == 1)]\n",
        "\n",
        "#training data\n",
        "#from keras.preprocessing.image import ImageDataGenerator\n",
        "#datagen = ImageDataGenerator(rescale=1./255)\n",
        "#train_bg = datagen.flow_from_directory('/content/drive/My Drive/New Data/train', target_size=(32, 32), shuffle=True, class_mode='binary')\n",
        "#x_ok, batchy = train_bg.next()\n",
        "pickle_in = open(\"jj.h\",\"rb\")\n",
        "X = pickle.load(pickle_in)\n",
        "X_tr = []\n",
        "X_trai = []\n",
        "X_train = []\n",
        "X_train1 = X['jet_image']\n",
        "X_train = X_train1[:80000]\n",
        "for i in range(0, len(X_tr)):\n",
        "  X_trai.append(preprocessing.normalize(X_tr[i]))\n",
        "\n",
        "\n",
        "#testing data\n",
        "#datagen = ImageDataGenerator(rescale=1./255)\n",
        "#train_sg = datagen.flow_from_directory('/content/drive/My Drive/New Data/test', target_size=(32, 32), class_mode='binary')\n",
        "#x_test, batchyt = train_sg.next()\n",
        "pickle_ins = open(\"ttx_had.h\",\"rb\")\n",
        "Y = pickle.load(pickle_ins)\n",
        "x_test1 = Y['jet_image']\n",
        "x_test = []\n",
        "x_te = []\n",
        "x_tes = []\n",
        "x_te = X_train1[80000:90000]\n",
        "xyz = x_test1[:10000]\n",
        "x_test = np.concatenate((x_te, xyz), axis=0)\n",
        "for i in range(0, len(x_te)):\n",
        "  x_tes.append(preprocessing.normalize(x_te[i]))\n",
        "label = []\n",
        "for i in range(10000):\n",
        "  label.append(0)\n",
        "for i in range(10000,20000):\n",
        "  label.append(1)\n",
        "\n",
        "\n",
        "#def reshape_x(x):\n",
        " #   new_x = np.empty((len(x), width, height))\n",
        "  #  for i, e in enumerate(x):\n",
        "   #     new_x[i] = cv2.resize(e, (width, height))\n",
        "    #return np.expand_dims(new_x, axis=-1) / 127 - 1\n",
        "  \n",
        "#x_ok = reshape_x(x_ok)\n",
        "#x_test = reshape_x(x_test)\n",
        "\n",
        "niter = 70\n",
        "bz = 128\n",
        "\n",
        "def get_data_generator(data, batch_size=128):\n",
        "    datalen = len(data)\n",
        "    cnt = 0\n",
        "    while True:\n",
        "        idxes = np.arange(datalen)\n",
        "        np.random.shuffle(idxes)\n",
        "        cnt += 1\n",
        "        for i in range(int(np.ceil(datalen/batch_size))):\n",
        "            train_x = np.take(data, idxes[i*batch_size: (i+1) * batch_size], axis=0)\n",
        "            y = np.ones(len(train_x))\n",
        "            yield train_x, y\n",
        "\n",
        "train_data_generator = get_data_generator(X_train, bz)\n",
        "\n",
        "for i in range(niter):\n",
        "    #break\n",
        "    ### get batch x, y ###\n",
        "    x, y = train_data_generator.__next__()\n",
        "    x = np.reshape(x, [bz, 32, 32, 1])\n",
        "        \n",
        "    ### train disciminator ###\n",
        "    d.trainable = True\n",
        "        \n",
        "    fake_x = g.predict(x)\n",
        "    \n",
        "    d_x = np.concatenate([x, fake_x], axis=0)\n",
        "    d_y = np.concatenate([np.zeros(len(x)), np.ones(len(fake_x))], axis=0)\n",
        "\n",
        "    d_loss = d.train_on_batch(d_x, d_y)\n",
        "\n",
        "    ### train generator ###\n",
        "    \n",
        "    #d.trainable = False        \n",
        "    #g_loss = gan_trainer.train_on_batch(x, y)\n",
        "    #g_loss = g.train_on_batch(x, y)\n",
        "    \n",
        "    #if i % 10 == 0:\n",
        "    print(f'niter: {i+1}, g_loss: , d_loss: {d_loss}')\n",
        "d.trainable = False\n",
        "X_train = np.reshape(X_train, [len(X_train), 32, 32, 1])\n",
        "g_loss = g.fit(X_train, X_train, batch_size=64, epochs=1, shuffle=True)\n",
        "print (g_loss)\n",
        "\n",
        "x_test = np.reshape(x_test, [20000, 32, 32, 1])\n",
        "encoded = g_e.predict(x_test)\n",
        "gan_x = g.predict(x_test)\n",
        "encoded_gan = encoder.predict(gan_x)\n",
        "score = []\n",
        "sc = []\n",
        "sc = np.absolute(encoded - encoded_gan)\n",
        "####################################################\n",
        "#-----------------------------------------------------#\n",
        "#score= np.sum(sc,axis=1)\n",
        "#score=score/(np.max(score)-np.min(score))\n",
        "#----------------------------------------------------#\n",
        "score_= np.sqrt(np.sum(sc**2,axis=1))\n",
        "score=(score_-np.min(score_))/(np.max(score_)-np.min(score_))\n",
        "print (np.min(score),np.max(score))\n",
        "###################################################\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "#p = g.predict(x_test)\n",
        "lr_fpr, lr_tpr, _ = roc_curve(label, score)\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "#from pylab import rcParams\n",
        "#rcParams['figure.figsize'] = 14, 5\n",
        "#plt.scatter(range(len(x_test)), score, c=['skyblue' if x == 0 else 'pink' for x in batchyt])\n",
        "\n",
        "\n",
        "plt.plot(lr_fpr, lr_tpr, marker='.', label='Ganomaly')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print (auc(lr_fpr, lr_tpr))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}